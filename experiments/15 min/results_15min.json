{
    "wikicat-es": {
        "accuracy_score": 0.6031746031746031,
        "precision_score": {
            "macro": 0.5294051944128882,
            "weighted": 0.6045957548673639
        },
        "recall_score": {
            "macro": 0.518118049054442,
            "weighted": 0.6031746031746031
        },
        "f1_score": {
            "macro": 0.5090073985876561,
            "weighted": 0.591760897373047
        },
        "balanced_accuracy_score": 0.518118049054442
    },
    "sst-en": {
        "accuracy_score": 0.5778280542986425,
        "precision_score": {
            "macro": 0.5766296198420119,
            "weighted": 0.5785906368940372
        },
        "recall_score": {
            "macro": 0.5455230769305471,
            "weighted": 0.5778280542986425
        },
        "f1_score": {
            "macro": 0.5505884066267441,
            "weighted": 0.568195414362693
        },
        "balanced_accuracy_score": 0.5455230769305471
    },
    "vaccine-es": {
        "accuracy_score": 0.7795389048991355,
        "precision_score": {
            "macro": 0.7603140549937429,
            "weighted": 0.7818540426822281
        },
        "recall_score": {
            "macro": 0.7668244785236428,
            "weighted": 0.7795389048991355
        },
        "f1_score": {
            "macro": 0.7630928484086672,
            "weighted": 0.780374876661097
        },
        "balanced_accuracy_score": 0.7668244785236428
    },
    "vaccine-en": {
        "accuracy_score": 0.5576923076923077,
        "precision_score": {
            "macro": 0.5686240029206223,
            "weighted": 0.5962412156182604
        },
        "recall_score": {
            "macro": 0.5325155505036153,
            "weighted": 0.5576923076923077
        },
        "f1_score": {
            "macro": 0.5291029534594759,
            "weighted": 0.5584280915109068
        },
        "balanced_accuracy_score": 0.5325155505036153
    },
    "women-clothing": {
        "accuracy_score": 0.7603352748970024,
        "precision_score": {
            "macro": 0.6326130424700471,
            "weighted": 0.7721252742711601
        },
        "recall_score": {
            "macro": 0.5511089487485118,
            "weighted": 0.7603352748970024
        },
        "f1_score": {
            "macro": 0.5587448972962705,
            "weighted": 0.7556978502763801
        },
        "balanced_accuracy_score": 0.6429604402065969
    },
    "inferes": {
        "accuracy_score": 0.7673697270471465,
        "precision_score": {
            "macro": 0.7586466086205353,
            "weighted": 0.7659390422298133
        },
        "recall_score": {
            "macro": 0.7596397233268742,
            "weighted": 0.7673697270471465
        },
        "f1_score": {
            "macro": 0.759076019318844,
            "weighted": 0.7665848161514103
        },
        "balanced_accuracy_score": 0.7596397233268742
    },
    "predict-salary": {
        "accuracy_score": 0.19103166186941373,
        "precision_score": {
            "macro": 0.1710177307445192,
            "weighted": 0.1843887568646051
        },
        "recall_score": {
            "macro": 0.17253665669209214,
            "weighted": 0.19103166186941373
        },
        "f1_score": {
            "macro": 0.16941377238482638,
            "weighted": 0.18504650617289345
        },
        "balanced_accuracy_score": 0.1725366566920921
    },
    "language-identification": {
        "accuracy_score": 0.9761,
        "precision_score": {
            "macro": 0.9778737932167687,
            "weighted": 0.9778737932167684
        },
        "recall_score": {
            "macro": 0.9761,
            "weighted": 0.9761
        },
        "f1_score": {
            "macro": 0.9759840861809748,
            "weighted": 0.9759840861809748
        },
        "balanced_accuracy_score": 0.9761
    },
    "paws-x-en": {
        "accuracy_score": 0.9525,
        "precision_score": {
            "micro": 0.9525,
            "normal": 0.9298150163220892
        },
        "recall_score": {
            "micro": 0.9525,
            "normal": 0.9655367231638418
        },
        "f1_score": {
            "micro": 0.9525,
            "normal": 0.9473392461197339
        },
        "balanced_accuracy_score": 0.9538445947657774
    },
    "paws-x-es": {
        "accuracy_score": 0.88975,
        "precision_score": {
            "micro": 0.88975,
            "normal": 0.8698591549295774
        },
        "recall_score": {
            "micro": 0.88975,
            "normal": 0.8802736602052451
        },
        "f1_score": {
            "micro": 0.88975,
            "normal": 0.8750354207990931
        },
        "balanced_accuracy_score": 0.8887120749824089
    },
    "wnli-es": {
        "accuracy_score": 0.44285714285714284,
        "precision_score": {
            "micro": 0.44285714285714284,
            "normal": 0.44285714285714284
        },
        "recall_score": {
            "micro": 0.44285714285714284,
            "normal": 1.0
        },
        "f1_score": {
            "micro": 0.44285714285714284,
            "normal": 0.6138613861386139
        },
        "balanced_accuracy_score": 0.5
    },
    "stroke-prediction": {
        "accuracy_score": 0.9510763209393346,
        "precision_score": {
            "micro": 0.9510763209393346,
            "normal": 0.0
        },
        "recall_score": {
            "micro": 0.9510763209393346,
            "normal": 0.0
        },
        "f1_score": {
            "micro": 0.9510763209393346,
            "normal": 0.0
        },
        "balanced_accuracy_score": 0.5
    },
    "project-kickstarter": {
        "accuracy_score": 0.5025919798314031,
        "precision_score": {
            "micro": 0.5025919798314031,
            "normal": 0.5108540925266903
        },
        "recall_score": {
            "micro": 0.5025919798314031,
            "normal": 0.09059640265067845
        },
        "f1_score": {
            "micro": 0.5025919798314031,
            "normal": 0.15389975877780757
        },
        "balanced_accuracy_score": 0.5020409235912715
    },
    "haha": {
        "accuracy_score": 0.8065,
        "precision_score": {
            "micro": 0.8065,
            "normal": 0.7105169340463459
        },
        "recall_score": {
            "micro": 0.8065,
            "normal": 0.8509820666097353
        },
        "f1_score": {
            "micro": 0.8065,
            "normal": 0.7744317077909463
        },
        "balanced_accuracy_score": 0.8145014214951356
    },
    "sentiment-lexicons-es": {
        "accuracy_score": 0.795,
        "precision_score": {
            "micro": 0.795,
            "normal": 0.8554216867469879
        },
        "recall_score": {
            "micro": 0.795,
            "normal": 0.71
        },
        "f1_score": {
            "micro": 0.795,
            "normal": 0.7759562841530054
        },
        "balanced_accuracy_score": 0.7949999999999999
    },
    "twitter-human-bots": {
        "accuracy_score": 0.8309294871794872,
        "precision_score": {
            "micro": 0.8309294871794872,
            "normal": 0.7696998123827392
        },
        "recall_score": {
            "micro": 0.8309294871794872,
            "normal": 0.6792218543046358
        },
        "f1_score": {
            "micro": 0.8309294871794872,
            "normal": 0.7216358839050132
        },
        "balanced_accuracy_score": 0.791207930306892
    },
    "fraudulent-jobs": {
        "accuracy_score": 0.9684936614466816,
        "precision_score": {
            "micro": 0.9684936614466816,
            "normal": 0.8461538461538461
        },
        "recall_score": {
            "micro": 0.9684936614466816,
            "normal": 0.6452513966480447
        },
        "f1_score": {
            "micro": 0.9684936614466816,
            "normal": 0.7321711568938192
        },
        "balanced_accuracy_score": 0.8184307322832712
    },
    "spanish-wine": {
        "mean_squared_error": {
            "RMSE": 9.922474660971805,
            "MSE": 98.45550339762754
        },
        "mean_absolute_error": 7.955002389526367
    },
    "price-book": {
        "mean_squared_error": {
            "RMSE": 599.679312598897,
            "MSE": 359615.2779590857
        },
        "mean_absolute_error": 432.64045392060893
    },
    "stsb-en": {
        "mean_squared_error": {
            "RMSE": 0.6635479766290717,
            "MSE": 0.44029591728853507
        },
        "mean_absolute_error": 0.49799926125752575
    },
    "stsb-es": {
        "mean_squared_error": {
            "RMSE": 0.9506574634447019,
            "MSE": 0.9037496128031148
        },
        "mean_absolute_error": 0.7089191195392328
    },
    "google-guest": {
        "mean_squared_error": {
            "RMSE": 0.4489532572135888,
            "MSE": 0.2015590271626908
        },
        "mean_absolute_error": 0.3615275184066356
    },
    "pub-health": {
        "accuracy_score": 0.7599351175993512,
        "precision_score": {
            "macro": 0.6505693181818182,
            "weighted": 0.7478202610042026
        },
        "recall_score": {
            "macro": 0.5760678458187237,
            "weighted": 0.7599351175993512
        },
        "f1_score": {
            "macro": 0.5933062010730729,
            "weighted": 0.7502538804103377
        },
        "balanced_accuracy_score": 0.5760678458187237
    },
    "trec": {
        "accuracy_score": 0.97,
        "precision_score": {
            "macro": 0.9476044341182348,
            "weighted": 0.9704990450072644
        },
        "recall_score": {
            "macro": 0.9726055886606817,
            "weighted": 0.97
        },
        "f1_score": {
            "macro": 0.9584455284151369,
            "weighted": 0.9699166846183179
        },
        "balanced_accuracy_score": 0.9726055886606817
    }
}
